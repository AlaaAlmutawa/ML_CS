{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Inference using Decode-only Model**\n",
        "\n",
        "This notebook was ran on Colab for easy of use. <br>\n",
        "The notebook expects: <br>\n",
        "- The trained model \"model.pkl.gz\" is placed in model folder. <br>\n",
        "- the trained tokenization model is placed in vocab_dir. <br>\n",
        "This notebook also includes the functions required to build the model architecture in preparation for evaluation. <br>"
      ],
      "metadata": {
        "id": "EJ8TEjtLKX5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "https://www.coursera.org/specializations/natural-language-processing\n",
        "https://github.com/LaurentVeyssier/TRAX_transformer_abstractive_summarization_model/blob/main/TRAX_transformer_summarizer_model.ipynb"
      ],
      "metadata": {
        "id": "f3spH2z67uJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install all the libraries**"
      ],
      "metadata": {
        "id": "WWKpdZ3M5QAX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfd0E-VI3lhc"
      },
      "outputs": [],
      "source": [
        "!pip install trax\n",
        "!pip install t5\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "U7EG-8oJ5Yk4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbxhyl_zFlWL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "import t5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running it on Colab, please mount and specify the correct path"
      ],
      "metadata": {
        "id": "tM6wdTE25dk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXfZd4qSVOge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0078d13-8712-4373-9da4-88673cef1ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cd \"/content/drive/MyDrive/ML project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQS8X5B93VFC"
      },
      "outputs": [],
      "source": [
        "import trax\n",
        "# Preprocess your data to prepare for Trax's TFDS structure\n",
        "# For example, create a list of tuples where each tuple contains your data\n",
        "prepared_data = [(article, abstract) for article, abstract in zip(train['article'], train['abstract'])]\n",
        "val_prepared_data = [(article, abstract) for article, abstract in zip(test['article'], test['abstract'])]\n",
        "\n",
        "\n",
        "def data_generator(data):\n",
        "    for text, label in data:\n",
        "        yield text, label\n",
        "\n",
        "# Create a generator from your prepared data\n",
        "train_data_stream = data_generator(prepared_data)\n",
        "val_data_stream = data_generator(val_prepared_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yez025XmkpO"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cEp3wmI2BX"
      },
      "outputs": [],
      "source": [
        "def tokenize(input_str, EOS=4):\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_dir='vocab_dir/',\n",
        "                                      vocab_file='spm_v4.model', ## change to the trained tokenization model\n",
        "                                      vocab_type='sentencepiece'))\n",
        "\n",
        "    return list(inputs)\n",
        "\n",
        "def detokenize(integers):\n",
        "    \"\"\"List of ints to str\"\"\"\n",
        "\n",
        "    s = trax.data.detokenize(integers,\n",
        "                             vocab_dir='vocab_dir/',\n",
        "                            vocab_file='spm_v4.model', ## change to the trained tokenization model\n",
        "                            vocab_type='sentencepiece')\n",
        "\n",
        "    return wrapper.fill(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjpYgzIhho_s"
      },
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"vocab_dir/spm_v4.model\")  # Load the trained model\n",
        "\n",
        "pad_id = sp.piece_to_id('<pad>')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjeJljA6hszP",
        "outputId": "f277318d-0fda-450c-fb4a-544392c46c81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkPb-UouiA-J"
      },
      "outputs": [],
      "source": [
        "eos_id = sp.piece_to_id('<EOS>')  # Get the ID of the <pad> token\n",
        "# Special tokens\n",
        "SEP = pad_id # Padding or separator token\n",
        "EOS = eos_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions build the transfomer model**"
      ],
      "metadata": {
        "id": "cSr8NuvG66yD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di5HQi_46sWy"
      },
      "outputs": [],
      "source": [
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ESm-aYR-tvx"
      },
      "outputs": [],
      "source": [
        "# DotProductAttention\n",
        "def DotProductAttention(query, key, value, mask):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    depth = query.shape[-1]\n",
        "    dots = jnp.matmul(query, jnp.swapaxes(key, -2, -1)) / jnp.sqrt(depth)\n",
        "    if mask is not None:\n",
        "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "    dots = jnp.exp(dots - logsumexp)\n",
        "    attention = jnp.matmul(dots, value)\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVyErXAei5_b"
      },
      "outputs": [],
      "source": [
        "# compute_attention_heads_closure\n",
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_heads function\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_attention_heads(x):\n",
        "        \"\"\" Compute the attention heads.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        seqlen = x.shape[1]\n",
        "        x = jnp.reshape(x,(batch_size, seqlen, n_heads, d_head))\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))\n",
        "\n",
        "        return x\n",
        "\n",
        "    return compute_attention_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryx9qx90jwXu"
      },
      "outputs": [],
      "source": [
        "# dot_product_self_attention\n",
        "def dot_product_self_attention(q, k, v):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (jax.interpreters.xla.DeviceArray): queries.\n",
        "        k (jax.interpreters.xla.DeviceArray): keys.\n",
        "        v (jax.interpreters.xla.DeviceArray): values.\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "    mask_size = q.shape[-2]\n",
        "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "\n",
        "\n",
        "    return DotProductAttention(q, k, v, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoizyBvLKv8h"
      },
      "outputs": [],
      "source": [
        "# compute_attention_output_closure\n",
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_output function\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_attention_output(x):\n",
        "        \"\"\" Compute the attention output.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        \"\"\"\n",
        "        seqlen = x.shape[1]\n",
        "        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "\n",
        "    return compute_attention_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma4o2nGdK5Xb"
      },
      "outputs": [],
      "source": [
        "def CausalAttention(d_feature,\n",
        "                    n_heads,\n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode='train'):\n",
        "    \"\"\"Transformer-style multi-headed causal attention.\n",
        "\n",
        "    Args:\n",
        "        d_feature (int):  dimensionality of feature embedding.\n",
        "        n_heads (int): number of attention heads.\n",
        "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "        dot_product_self_attention (function): dot_product_self_attention function.\n",
        "        compute_attention_output_closure (function): Closure around compute_attention_output.\n",
        "        mode (str): 'train' or 'eval'.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "    \"\"\"\n",
        "\n",
        "    assert d_feature % n_heads == 0\n",
        "    d_head = d_feature // n_heads\n",
        "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "\n",
        "\n",
        "    return tl.Serial(\n",
        "        tl.Branch(\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads],\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads],\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads],\n",
        "        ),\n",
        "\n",
        "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1),\n",
        "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1),\n",
        "        tl.Dense(d_feature),\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALMwKMx--ZF7"
      },
      "outputs": [],
      "source": [
        "# DecoderBlock\n",
        "def DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "    causal_attention = CausalAttention(\n",
        "                        d_model,\n",
        "                        n_heads=n_heads,\n",
        "                        mode=mode\n",
        "                        )\n",
        "\n",
        "    feed_forward = [\n",
        "        tl.LayerNorm(),\n",
        "        tl.Dense(d_ff),\n",
        "        ff_activation(),\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        tl.Dense(d_model),\n",
        "        tl.Dropout(rate=dropout, mode=mode)\n",
        "    ]\n",
        "\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          tl.LayerNorm(),\n",
        "          causal_attention,\n",
        "          tl.Dropout(rate=dropout, mode=mode),\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          feed_forward\n",
        "        ),\n",
        "      ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEyUBeu7ACRt"
      },
      "outputs": [],
      "source": [
        "# TransformerLM\n",
        "def TransformerLM(vocab_size=32000,\n",
        "                  d_model=512,\n",
        "                  d_ff=2048,\n",
        "                  n_layers=6,\n",
        "                  n_heads=8,\n",
        "                  dropout=0.1,\n",
        "                  max_len=4096,\n",
        "                  mode='train',\n",
        "                  ff_activation=tl.Relu):\n",
        "    \"\"\"Returns a Transformer language model.\n",
        "\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\n",
        "    decoder part of the overall Transformer.)\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_layers (int): number of decoder layers.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
        "        to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    positional_encoder = [\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
        "\n",
        "    decoder_blocks = [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    return tl.Serial(\n",
        "        tl.ShiftRight(mode=mode),\n",
        "        positional_encoder,\n",
        "        decoder_blocks,\n",
        "        tl.LayerNorm(),\n",
        "        tl.Dense(vocab_size),\n",
        "        tl.LogSoftmax()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh-DQ6xaC-zm"
      },
      "outputs": [],
      "source": [
        "# Get the model architecture\n",
        "model = TransformerLM(mode='eval')\n",
        "\n",
        "# Load the pre-trained weights ## specify the model name and path\n",
        "model.init_from_file('model/model.pkl.gz', weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKLpfkUBiNwW"
      },
      "outputs": [],
      "source": [
        "## helper function to get the next token from the model\n",
        "def next_symbol(cur_output_tokens, model):\n",
        "    \"\"\"Returns the next symbol for a given sentence.\n",
        "\n",
        "    Args:\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\n",
        "\n",
        "    Returns:\n",
        "        int: tokenized symbol.\n",
        "    \"\"\"\n",
        "    token_length = len(cur_output_tokens)\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
        "\n",
        "    padded = cur_output_tokens + [3] * (padded_length - token_length)\n",
        "    padded_with_batch = np.array(padded)[None, :]\n",
        "    output, _ = model((padded_with_batch, padded_with_batch))\n",
        "    log_probs = output[0, token_length, :]\n",
        "\n",
        "    return int(np.argmax(log_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3arzOJMXmqLL"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(input_sentence, model, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "    \"\"\"Greedy decode function.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (string): a sentence or article.\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        string: summary of the input.\n",
        "    \"\"\"\n",
        "\n",
        "    cur_output_tokens = tokenize(input_sentence) + [4,3]\n",
        "    generated_output = []\n",
        "    cur_output = 0\n",
        "    EOS = 4\n",
        "\n",
        "    while cur_output != EOS:\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\n",
        "        cur_output_tokens.append(cur_output)\n",
        "        generated_output.append(cur_output)\n",
        "\n",
        "        print(detokenize(generated_output))\n",
        "\n",
        "    return detokenize(generated_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we try to generate summary using the model**"
      ],
      "metadata": {
        "id": "t-JHgzVy7nQu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jr3ueFt7m1Xn",
        "outputId": "aaaa76f1-515b-4353-e450-bfba948477c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " patients with cervical radiculopathy due to single  level\n",
            "degenerative disc disease who fail to improve with nonoperative\n",
            "therapy are candidates for anterior decompression and reconstruction\n",
            "with either an arthrodesis or arthroplasty  both procedures require\n",
            "minimal hospitalization and are highly effective in relieving pain and\n",
            "improving neurological function  the ability to return to work and the\n",
            "speed with which this occurs are important to the individual being\n",
            "treated and also to society  arthrodesis and arthroplasty differ in\n",
            "that one treatment eliminates motion at a cervical spinal segment\n",
            "while the other preserves it  this fundamental difference may impact\n",
            "postoperative function in terms of activity level  which ultimately\n",
            "could facilitate or hinder the ability to return to work  in patients\n",
            "with degenerative disease of the cervical spine  does cervical\n",
            "artificial disc replacement lead to better work  related outcomes than\n",
            "fusion  does return to work after surgery differ based on gender  age\n",
            "smoking  l \n",
            "\n",
            "and\n",
            "and and\n",
            "and and and\n",
            "and and and and\n",
            "and and and and .\n",
            "and and and and . .\n",
            "and and and and . . and\n",
            "and and and and . . and and\n",
            "and and and and . . and and .\n",
            "and and and and . . and and . .\n",
            "and and and and . . and and . . .\n",
            "and and and and . . and and . . . .\n",
            "and and and and . . and and . . . . .\n",
            "and and and and . . and and . . . . . .\n",
            "and and and and . . and and . . . . . . and\n",
            "and and and and . . and and . . . . . . and and\n",
            "and and and and . . and and . . . . . . and and and\n",
            "and and and and . . and and . . . . . . and and and and\n",
            "and and and and . . and and . . . . . . and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and\n",
            "and and and and . . and and . . . . . . and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and and and and and and and and and and and and and and and and\n",
            "and and\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e8394c44570d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" patients with cervical radiculopathy due to single  level degenerative disc disease who fail to improve with nonoperative therapy are candidates for anterior decompression and reconstruction with either an arthrodesis or arthroplasty  both procedures require minimal hospitalization and are highly effective in relieving pain and improving neurological function  the ability to return to work and the speed with which this occurs are important to the individual being treated and also to society  arthrodesis and arthroplasty differ in that one treatment eliminates motion at a cervical spinal segment while the other preserves it  this fundamental difference may impact postoperative function in terms of activity level  which ultimately could facilitate or hinder the ability to return to work  in patients with degenerative disease of the cervical spine  does cervical artificial disc replacement lead to better work  related outcomes than fusion  does return to work after surgery differ based on gender  age  smoking  l\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-eba3b20058f8>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(input_sentence, model, next_symbol, tokenize, detokenize)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur_output\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEOS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Get next symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcur_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Append next symbol to original sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-51f5132c0458>\u001b[0m in \u001b[0;36mnext_symbol\u001b[0;34m(cur_output_tokens, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# model expects a tuple containing two padded tensors (with batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# HINT: output has shape (1, padded_length, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# To get log_probs you need to index output wih 0 in the first dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_from_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_onto_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# Note that zip silently truncates its result if lengths don't match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0msub_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_out\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mrngs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_rngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       return tuple(layer.state if s is None else s\n\u001b[0m\u001b[1;32m    471\u001b[0m                    for (layer, s) in zip(self.sublayers, self._state))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       return tuple(layer.state if s is None else s\n\u001b[0m\u001b[1;32m    471\u001b[0m                    for (layer, s) in zip(self.sublayers, self._state))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       return tuple(layer.state if s is None else s\n\u001b[0m\u001b[1;32m    471\u001b[0m                    for (layer, s) in zip(self.sublayers, self._state))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       return tuple(layer.state if s is None else s\n\u001b[0m\u001b[1;32m    471\u001b[0m                    for (layer, s) in zip(self.sublayers, self._state))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_sentence = \" patients with cervical radiculopathy due to single  level degenerative disc disease who fail to improve with nonoperative therapy are candidates for anterior decompression and reconstruction with either an arthrodesis or arthroplasty  both procedures require minimal hospitalization and are highly effective in relieving pain and improving neurological function  the ability to return to work and the speed with which this occurs are important to the individual being treated and also to society  arthrodesis and arthroplasty differ in that one treatment eliminates motion at a cervical spinal segment while the other preserves it  this fundamental difference may impact postoperative function in terms of activity level  which ultimately could facilitate or hinder the ability to return to work  in patients with degenerative disease of the cervical spine  does cervical artificial disc replacement lead to better work  related outcomes than fusion  does return to work after surgery differ based on gender  age  smoking  l\"\n",
        "print(wrapper.fill(test_sentence), '\\n')\n",
        "print(greedy_decode(test_sentence, model))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}