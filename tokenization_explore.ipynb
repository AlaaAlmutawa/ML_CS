{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety affects quality of life in those livin...</td>\n",
       "      <td>research on the implications of anxiety in pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small non  coding rnas are transcribed into mr...</td>\n",
       "      <td>small non  coding rnas include sirna  mirna  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ohss is a serious complication of ovulation in...</td>\n",
       "      <td>objective  to evaluate the efficacy and safet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>congenital adrenal hyperplasia  cah  refers to...</td>\n",
       "      <td>congenital adrenal hyperplasia is a group of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type 1 diabetes  t1d  results from the destruc...</td>\n",
       "      <td>objectivespentoxifylline is an immunomodulato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6653</th>\n",
       "      <td>neurocysticercosis is the most common parasiti...</td>\n",
       "      <td>cysticercosis is an infection with larval cys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>most chd prevalence data are based on populati...</td>\n",
       "      <td>background  congenital heart disease  chd  is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6655</th>\n",
       "      <td>renal transplantation rates are low among pati...</td>\n",
       "      <td>we describe a successful living donor liver t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>acute kidney injury  aki   defined as an abrup...</td>\n",
       "      <td>objective  the aim was to investigate the eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>human brain plasticity or neuroplasticity refe...</td>\n",
       "      <td>accumulating evidence has indicated that ampu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6658 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "0     anxiety affects quality of life in those livin...   \n",
       "1     small non  coding rnas are transcribed into mr...   \n",
       "2     ohss is a serious complication of ovulation in...   \n",
       "3     congenital adrenal hyperplasia  cah  refers to...   \n",
       "4     type 1 diabetes  t1d  results from the destruc...   \n",
       "...                                                 ...   \n",
       "6653  neurocysticercosis is the most common parasiti...   \n",
       "6654  most chd prevalence data are based on populati...   \n",
       "6655  renal transplantation rates are low among pati...   \n",
       "6656  acute kidney injury  aki   defined as an abrup...   \n",
       "6657  human brain plasticity or neuroplasticity refe...   \n",
       "\n",
       "                                               abstract  \n",
       "0      research on the implications of anxiety in pa...  \n",
       "1      small non  coding rnas include sirna  mirna  ...  \n",
       "2      objective  to evaluate the efficacy and safet...  \n",
       "3      congenital adrenal hyperplasia is a group of ...  \n",
       "4      objectivespentoxifylline is an immunomodulato...  \n",
       "...                                                 ...  \n",
       "6653   cysticercosis is an infection with larval cys...  \n",
       "6654   background  congenital heart disease  chd  is...  \n",
       "6655   we describe a successful living donor liver t...  \n",
       "6656   objective  the aim was to investigate the eff...  \n",
       "6657   accumulating evidence has indicated that ampu...  \n",
       "\n",
       "[6658 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('clean_test_hugging_face_scientific.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['article'].tolist()\n",
    "# tokenizer.train(trainer, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6658"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.extend(df['abstract'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13316"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, trainers, normalizers, pre_tokenizers, models,processors  \n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Set up the tokenizer with necessary settings (e.g., normalizers, pre-tokenizers)\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2)\n",
    "tokenizer.train_from_iterator(text, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = tokenizer.model.save(\"data\", \"scientific_pubmed\")\n",
    "# tokenizer.model = BPE.from_file(*files, unk_token=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/tokenizer-scientific_pubmed.json\")\n",
    "# tokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size of the trained tokenizer is: 10000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"The vocabulary size of the trained tokenizer is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trax\n",
    "# Preprocess your data to prepare for Trax's TFDS structure\n",
    "# For example, create a list of tuples where each tuple contains your data\n",
    "prepared_data = [(article, abstract) for article, abstract in zip(train['article'], train['abstract'])]\n",
    "val_prepared_data = [(article, abstract) for article, abstract in zip(test['article'], test['abstract'])]\n",
    "\n",
    "\n",
    "def data_generator(data):\n",
    "    for text, label in data:\n",
    "        yield text, label\n",
    "\n",
    "# Create a generator from your prepared data\n",
    "train_data_stream = data_generator(prepared_data)\n",
    "val_data_stream = data_generator(val_prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing function using Trax Tokenizer\n",
    "def tokenize_text(text,tokenizer=tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return list(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = df['article'].apply(tokenize_text)\n",
    "tokenized_abstracts = df['abstract'].apply(tokenize_text)\n",
    "\n",
    "# Creating a new DataFrame with tokenized data\n",
    "df_tokenized = pd.DataFrame({\n",
    "    'tokenized_article': tokenized_articles,\n",
    "    'tokenized_abstract': tokenized_abstracts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_text(token_ids, tokenizer=tokenizer):\n",
    "    decoded_text = tokenizer.decode(token_ids)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_articles = df_tokenized['tokenized_article'].apply(detokenize_text)\n",
    "detokenized_abstracts = df_tokenized['tokenized_abstract'].apply(detokenize_text)\n",
    "\n",
    "# Creating a new DataFrame with tokenized data\n",
    "df_detokenized = pd.DataFrame({\n",
    "    'detokenized_article': detokenized_articles,\n",
    "    'detokenized_abstract': detokenized_abstracts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detokenized_article</th>\n",
       "      <th>detokenized_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety affects quality of life in those livin...</td>\n",
       "      <td>research on the implications of anxiety in par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small non coding rnas are transcri bed into mr...</td>\n",
       "      <td>small non coding rnas include sirna mirna p ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh ss is a serious complication of ov ulation ...</td>\n",
       "      <td>objective to evaluate the efficacy and safety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>congenital adrenal hyperplasia ca h refers to ...</td>\n",
       "      <td>congenital adrenal hyperplasia is a group of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type 1 diabetes t1d results from the destructi...</td>\n",
       "      <td>objectives pen tox ify ll ine is an immuno mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6653</th>\n",
       "      <td>neuro cystic er co sis is the most common para...</td>\n",
       "      <td>cystic er co sis is an infection with lar val ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>most chd prevalence data are based on populati...</td>\n",
       "      <td>background congenital heart disease chd is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6655</th>\n",
       "      <td>renal transplantation rates are low among pati...</td>\n",
       "      <td>we describe a successful living donor liver tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>acute kidney injury aki defined as an ab rup t...</td>\n",
       "      <td>objective the aim was to investigate the effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>human brain plasticity or neuro plasticity ref...</td>\n",
       "      <td>accum ulating evidence has indicated that amp ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6658 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    detokenized_article  \\\n",
       "0     anxiety affects quality of life in those livin...   \n",
       "1     small non coding rnas are transcri bed into mr...   \n",
       "2     oh ss is a serious complication of ov ulation ...   \n",
       "3     congenital adrenal hyperplasia ca h refers to ...   \n",
       "4     type 1 diabetes t1d results from the destructi...   \n",
       "...                                                 ...   \n",
       "6653  neuro cystic er co sis is the most common para...   \n",
       "6654  most chd prevalence data are based on populati...   \n",
       "6655  renal transplantation rates are low among pati...   \n",
       "6656  acute kidney injury aki defined as an ab rup t...   \n",
       "6657  human brain plasticity or neuro plasticity ref...   \n",
       "\n",
       "                                   detokenized_abstract  \n",
       "0     research on the implications of anxiety in par...  \n",
       "1     small non coding rnas include sirna mirna p ir...  \n",
       "2     objective to evaluate the efficacy and safety ...  \n",
       "3     congenital adrenal hyperplasia is a group of a...  \n",
       "4     objectives pen tox ify ll ine is an immuno mod...  \n",
       "...                                                 ...  \n",
       "6653  cystic er co sis is an infection with lar val ...  \n",
       "6654  background congenital heart disease chd is the...  \n",
       "6655  we describe a successful living donor liver tr...  \n",
       "6656  objective the aim was to investigate the effic...  \n",
       "6657  accum ulating evidence has indicated that amp ...  \n",
       "\n",
       "[6658 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length in 'article' and 'abstract' columns is: 7959\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df_tokenized' contains your tokenized sequences\n",
    "\n",
    "max_length = 0\n",
    "\n",
    "for index, row in df_tokenized.iterrows():\n",
    "    article_length = len(row['tokenized_article'])\n",
    "    abstract_length = len(row['tokenized_abstract'])\n",
    "    \n",
    "    max_length = max(max_length, article_length, abstract_length)\n",
    "\n",
    "print(f\"The maximum sequence length in 'article' and 'abstract' columns is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples with a sequence length of 2048 is: 2431\n"
     ]
    }
   ],
   "source": [
    "count_2048 = 0\n",
    "\n",
    "for index, row in df_tokenized.iterrows():\n",
    "    article_length = len(row['tokenized_article'])\n",
    "    abstract_length = len(row['tokenized_abstract'])\n",
    "    \n",
    "    if article_length <= 2048 and abstract_length <= 2048:\n",
    "        count_2048 += 1\n",
    "\n",
    "print(f\"The number of examples with a sequence length of 2048 is: {count_2048}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### too long of a sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### sentencePiece tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, trainers, normalizers, pre_tokenizers, models,processors  \n",
    "\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase()])\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    text,\n",
    "    vocab_size=10_000,\n",
    "    min_frequency=10,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")\n",
    "\n",
    "# trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2)\n",
    "# tokenizer.train_from_iterator(text, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp311-cp311-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.1 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = tokenizer.model.save(\"data\", \"sentencePiece_scientific_pubmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/sentencePiece_tokenizer-scientific_pubmed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing function using Trax Tokenizer\n",
    "def tokenize_text(text,tokenizer=tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return list(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = df['article'].apply(tokenize_text)\n",
    "tokenized_abstracts = df['abstract'].apply(tokenize_text)\n",
    "\n",
    "# Creating a new DataFrame with tokenized data\n",
    "df2_tokenized = pd.DataFrame({\n",
    "    'tokenized_article': tokenized_articles,\n",
    "    'tokenized_abstract': tokenized_abstracts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length in 'article' and 'abstract' columns is: 7959\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df_tokenized' contains your tokenized sequences\n",
    "\n",
    "max_length = 0\n",
    "\n",
    "for index, row in df2_tokenized.iterrows():\n",
    "    article_length = len(row['tokenized_article'])\n",
    "    abstract_length = len(row['tokenized_abstract'])\n",
    "    \n",
    "    max_length = max(max_length, article_length, abstract_length)\n",
    "\n",
    "print(f\"The maximum sequence length in 'article' and 'abstract' columns is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.15.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from tokenizers) (0.20.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/tf_ml_project/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n",
      "Downloading tokenizers-0.15.0-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
